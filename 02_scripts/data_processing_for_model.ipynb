{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing for model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and process dependent variable - turnover"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read turnover data from csv into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual and monthly data\n",
    "annual_url = '../01_data/processed_annual_turnover.csv'\n",
    "# monthly_url = '../01_data/processed_monthly_turnover.csv'\n",
    "\n",
    "# staff group ref table\n",
    "ref_sg = pd.read_csv('../01_data/ref_sg_grouped.csv')\n",
    "\n",
    "annual_df = pd.read_csv(annual_url, parse_dates=['month_year'])\n",
    "annual_df = annual_df.drop(['n'],axis=1)\n",
    "annual_df.info()\n",
    "\n",
    "# monthly_df = pd.read_csv(monthly_url, parse_dates=['month_year'])\n",
    "# monthly_df = monthly_df.drop(['n'],axis=1)\n",
    "# monthly_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to clean and process DV dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dv(df, ref_sg):\n",
    "    # drop org_type because it has historic variation which is creating duplicates when mapping later\n",
    "    df.drop('org_type', axis=1, inplace=True) \n",
    "    # add leaver and joiner rates column\n",
    "    df['leaver_rate'] = df['leave_FTE']/df['denom_FTE_average']\n",
    "    df['joiner_rate'] = df['join_FTE']/df['denom_FTE_average']\n",
    "\n",
    "    # drop unneeded HC columns\n",
    "    df = df.drop(['join_HC','leave_HC','denom_HC','denom_FTE_start',\n",
    "                    'denom_HC_start','denom_FTE_average','denom_HC_average'],axis=1)\n",
    "    # drop 'All staff groups' - use sum of all others instead when wanting all staff.\n",
    "    #df.drop(df[df['staff_group'] == 'All staff groups'].index, inplace = True)\n",
    "\n",
    "    # group staff groups with mapping table\n",
    "    df = pd.merge(df, ref_sg, on='staff_group', how='left')\n",
    "\n",
    "    ## calculate columns with % of staff groups by organisation and date for use in model\n",
    "    # first, calculate a total staff in post (SIP; all staff) FTE dataframe for each organisation by month.\n",
    "    df_sip_org = df.groupby(['month_year', 'org_code', 'staff_group','region_name'])['denom_FTE'].sum().reset_index()\n",
    "    # second, group by 'month_year' and 'org_code' and sum the 'FTE' values for each group\n",
    "    df_total_sip_FTE = df_sip_org.groupby(['month_year', 'org_code','region_name'])['denom_FTE'].sum().reset_index()\n",
    "    # third, merge the total_sip_FTE DataFrame back into the original DataFrame \n",
    "    df2 = pd.merge(df, df_total_sip_FTE, on=['month_year', 'org_code','region_name'], suffixes=('', '_total'))\n",
    "\n",
    "    ## (also calculate here a regional sip FTE by staff group column for later use)\n",
    "    # staff in post by staff group for each region by month\n",
    "    df_sip_region = df.groupby(['month_year','staff_group','region_name'])['denom_FTE'].sum().reset_index()\n",
    "    # # # merge the df_sip_region DataFrame  \n",
    "    df2 = pd.merge(df2, df_sip_region, on=['month_year','region_name','staff_group'], suffixes=('', '_region'))\n",
    "\n",
    "    df2.rename(columns={'denom_FTE_total': 'total_sip_FTE','denom_FTE_region':'sip_FTE_region'}, inplace=True)\n",
    "\n",
    "    # fourth, calculate the percentage of staff group FTE by organization and date\n",
    "    df2['%_FTE'] = df2['denom_FTE'] / df2['total_sip_FTE']\n",
    "\n",
    "    # fifth, pivot the DataFrame to get staff groups as new columns with % values\n",
    "    df3 = df2.pivot(index=['month_year', 'org_code','region_name'], \n",
    "                    columns='staff_group', values='%_FTE').reset_index()\n",
    "\n",
    "    # finally, merge the pivot DataFrame with the original DataFrame\n",
    "    df4 = pd.merge(df2, df3, on=['month_year', 'org_code','region_name'])\n",
    "\n",
    "    # # make the new staff group name columns friendlier\n",
    "    df4.rename(columns={'Ambulance staff': 'amb_staff','Central functions':'cent_funct',\n",
    "    'HCHS doctors (exc. junior Drs)': 'senior_docs',\n",
    "    'Hotel, property & estates': 'estates', 'Managers':'managers','Nurses & health visitors': 'nurses_hv',\n",
    "    'Other staff or those with unknown classification':'unknown',\n",
    "    'Scientific, therapeutic & technical staff':'sci_tech_staff',\n",
    "    'Senior managers':'senior_managers','Support to ST&T staff': 'supp_sci_tech',\n",
    "    'Support to doctors, nurses & midwives': 'supp_doc_nur_mid',\n",
    "    'Midwives':'midwives','Support to ambulance staff': 'supp_amb_staff'}, inplace=True)\n",
    "\n",
    "    # drop small staff groups & non sig in regression:\n",
    "    df4.drop(['All staff groups','supp_amb_staff','senior_managers','managers',\n",
    "                'amb_staff','unknown','cent_funct'], axis=1, inplace=True) \n",
    "\n",
    "    # replace inf values with nan (can happen with rate calcs)\n",
    "    df4.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    # transform nans to zeros\n",
    "    df4.fillna(0, inplace=True)\n",
    "\n",
    "    # Add a small constant to avoid taking the log of zero\n",
    "    small_constant = 1e-5\n",
    "    \n",
    "    # log scale the total_SIP_FTE column to be in line with other variables. proxy for size of organisation\n",
    "    df4['log_total_sip_FTE'] = np.log(df4['total_sip_FTE'] + small_constant)\n",
    "\n",
    "    # drop unused columns (keep total SIP FTE for calculating vacancy rates later)\n",
    "    df4.drop(['join_FTE','leave_FTE','%_FTE','denom_FTE','total_sip_FTE'], axis=1, inplace=True)\n",
    "\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df1 = clean_dv(annual_df, ref_sg)\n",
    "#monthly_df1 = clean_dv(monthly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data show the full time equivalent (FTE) number of leavers by organisation and staff group for the previous 12-month period from the date. It also shows the number of staff in post (SIP) FTE averaged over the 12-month period to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(annual_df1['staff_group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load independent variable 1 - local unemployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data about local unemployment so we can use it as a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r1 = '../01_data/ONS_localunemployment_monthly.csv'\n",
    "df_r1 = pd.read_csv(url_r1, parse_dates=['Date'])\n",
    "\n",
    "df_r1.rename(columns={'%':'local_unemployment','Date':'month_year',\n",
    "                      'NHSE region name':'region_name'},inplace=True)\n",
    "df_r1 = df_r1.sort_values('month_year')\n",
    "df_r1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IV 2 - sickness absence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data about sickness absence to use as second regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r2 = '../01_data/sickness_benchmarking.csv'\n",
    "df_r2 = pd.read_csv(url_r2, parse_dates=['DATE'])\n",
    "trust_types_todrop = ['Clinical Commissioning Group','Integrated Care Board']\n",
    "df_r2 = df_r2[~df_r2['CLUSTER_GROUP'].isin(trust_types_todrop)]\n",
    "df_r2 = df_r2.drop(['BENCHMARK_GROUP','ORG_NAME',\n",
    "                     'NHSE_REGION_CODE','CLUSTER_GROUP'],axis=1)\n",
    "df_r2.rename(columns={'ORG_CODE':'org_code','DATE':'month_year',\n",
    "                       'NHSE_REGION_NAME':'region_name','STAFF_GROUP':'staff_group',\n",
    "                       'FTE_DAYS_LOST':'fte_days_lost','FTE_DAYS_AVAILABLE':'fte_days_available'},inplace=True)\n",
    "merge_cols = ['month_year', 'org_code','region_name','staff_group']\n",
    "df_r2['sickness_absence'] = df_r2['fte_days_lost']/df_r2['fte_days_available']\n",
    "df_r2 = df_r2.reset_index(drop=True)\n",
    "df_r2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 12-month rolling sickness absence column for use with annual turnover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2['month_year'] = pd.to_datetime(df_r2['month_year'])\n",
    "\n",
    "# Sort the DataFrame by organisation, staff_group, and month\n",
    "df_r2.sort_values(by=['month_year'], inplace=True)\n",
    "\n",
    "# # Calculate the rolling sums for days lost and days available\n",
    "# df_r2['rolling_days_lost'] = df_r2.groupby(['org_code', \n",
    "#                         'staff_group'])['fte_days_lost'].rolling(window=12, min_periods=1).sum().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# df_r2['rolling_days_available'] = df_r2.groupby(['org_code', \n",
    "#                         'staff_group'])['fte_days_available'].rolling(window=12, min_periods=1).sum().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# # Calculate the rolling sickness absence rate\n",
    "# df_r2['annual_sickness_absence'] = df_r2['rolling_days_lost'] / df_r2['rolling_days_available']\n",
    "\n",
    "# drop fte_days_lost fte_days_available, rolling_days_available and rolling_days_lost columns\n",
    "df_r2.drop(columns=['fte_days_lost', 'fte_days_available'], inplace=True)\n",
    "#, 'rolling_days_available', 'rolling_days_lost'], inplace=True)\n",
    "\n",
    "df_r2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IV 3 - reasons for sickness absence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data about reasons for sickness absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r3 = '../01_data/sickness_absence_reason_pivot.csv'\n",
    "df_r3 = pd.read_csv(url_r3, parse_dates=['Date'])\n",
    "#df_r3 = df_r3.drop(['FTE days lost'],axis=1)\n",
    "df_r3.rename(columns={'Date':'month_year','Staff group':'staff_group'},inplace=True)\n",
    "#df_r2 = df_r2.reset_index(drop=True)\n",
    "\n",
    "# drop least frequent reasons for absence\n",
    "df_r3 = df_r3.drop(['substance_abus','asthma',\n",
    "                    'dental','blood_disorder','endocrine',\n",
    "                    'eye','skin_disorders','nervous_system',\n",
    "                    'gynaecological','unknown','pregnancy_related',\n",
    "                    'other'],axis=1)\n",
    "\n",
    "# Replace NaN values with 0 \n",
    "df_r3 = df_r3.fillna(0)\n",
    "df_r3.info()\n",
    "# national level data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IV 4 and 5 - staff vacancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_sg_ref = '../01_data/ref_sg_vacancy.csv'\n",
    "df_sg_ref = pd.read_csv(url_sg_ref)\n",
    "df_sg_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r4 = '../01_data/vacancy_ESR.csv'\n",
    "df_r4 = pd.read_csv(url_r4,parse_dates=['month_year'],dayfirst=True)\n",
    "\n",
    "df_r4 = df_r4.drop(['Published month','Published quarter','England'],axis=1)\n",
    "df_r4.rename(columns={'NWD Staff Group':'vacancy_sg','NHS England region':'region_name',\n",
    "                        'Vacancy Wte':'vacancy_FTE'},inplace=True)\n",
    "\n",
    "df_r4 = df_r4.fillna(0)\n",
    "\n",
    "# Remove code in brackets\n",
    "df_r4['region_name'] = df_r4['region_name'].str[:-6].str.rstrip()\n",
    "\n",
    "# Add staff groupings to match other datasets\n",
    "df_r4 = pd.merge(df_r4, df_sg_ref, on='vacancy_sg',how='left')\n",
    "\n",
    "df_r4 = df_r4.drop(['all'],axis=1)\n",
    "\n",
    "df_r4.info()\n",
    "\n",
    "# regional level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r5 = '../01_data/vacancy_TRAC.csv'\n",
    "df_r5 = pd.read_csv(url_r5,parse_dates=['month_year'],dayfirst=True)\n",
    "\n",
    "df_r5 = df_r5.drop(['Published month','Published quarter','England'],axis=1)\n",
    "\n",
    "df_r5.rename(columns={'NWD Staff Group':'vacancy_sg','NHS England region':'region_name',\n",
    "                        'Advertised FTE':'advertised_FTE'},inplace=True)\n",
    "\n",
    "df_r5 = df_r5.fillna(0)\n",
    "\n",
    "# Remove region code in brackets\n",
    "df_r5['region_name'] = df_r5['region_name'].str[:-6].str.rstrip()\n",
    "\n",
    "# Add staff groupings to match other datasets\n",
    "df_r5 = pd.merge(df_r5, df_sg_ref, on='vacancy_sg',how='left')\n",
    "\n",
    "df_r5 = df_r5.drop(['all'],axis=1)\n",
    "\n",
    "# regional level\n",
    "\n",
    "df_r5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IV 6 - Reasons for leaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r6 = '../01_data/rfl_jun23.csv'\n",
    "df_r6_a = pd.read_csv(url_r6,parse_dates=['month_year'],dayfirst=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r6_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = ['month_year','%_neg_RFL','%_dismissal','%_end_of_ft',\n",
    "            '%_flexibility','%_health','%_pay_reward','%_progression_cpd',\n",
    "            '%_relocation','%_retirement','%_work_life_balance','%_workforce_transform']\n",
    "df_r6 = df_r6_a[to_keep]\n",
    "#df_r6 = df_r6_a\n",
    "df_r6.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge IV dfs to main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(annual_df1['staff_group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ivs(df, df_r1, df_r2, df_r3,df_r4,df_r5,df_r6):\n",
    "    # local unemployment rate\n",
    "    df1 = pd.merge(df, df_r1, on=['month_year', 'region_name'],how='left')\n",
    "    df1 = df1.sort_values('month_year')\n",
    "\n",
    "    # sickness absence\n",
    "    r2_merge_cols = ['month_year', 'org_code','region_name','staff_group']\n",
    "    df2 = pd.merge(df1, df_r2, on=r2_merge_cols,how='left')\n",
    "    #df2.drop_duplicates(subset=r2_merge_cols)\n",
    "\n",
    "    # reason for sickness absence\n",
    "    r3_merge_cols = ['month_year','staff_group']\n",
    "    df3 = pd.merge(df2, df_r3, on=r3_merge_cols,how='left')\n",
    "\n",
    "    # vacancy - need to calculate rate at regional level here\n",
    "    # use sip_FTE_region calculated earlier\n",
    "    r4_merge_cols = ['month_year','region_name','staff_group']\n",
    "\n",
    "    df4 = pd.merge(df3, df_r4, on=r4_merge_cols,how='left')\n",
    "\n",
    "    df4.drop(columns=['vacancy_sg'], inplace=True)\n",
    "\n",
    "    df4['vacancy_rate'] = df4['vacancy_FTE'] / df4['sip_FTE_region']\n",
    "\n",
    "    df5 = pd.merge(df4, df_r5, on=r4_merge_cols,how='left')\n",
    "\n",
    "    df5.drop(columns=['vacancy_sg'], inplace=True)\n",
    "\n",
    "    df5['advertised_rate'] = df5['advertised_FTE'] / df5['sip_FTE_region']\n",
    "\n",
    "    df5.drop(columns=['sip_FTE_region','advertised_FTE','vacancy_FTE'], inplace=True)\n",
    "\n",
    "    # reasons for leaving\n",
    "    df6 = pd.merge(df5, df_r6, on='month_year',how='left')\n",
    "\n",
    "    # add region as dummy variable\n",
    "    df6 = pd.get_dummies(df6, columns=['region_name'], drop_first=True)\n",
    "\n",
    "    # add org type as dummy variable\n",
    "#    df6 = pd.get_dummies(df6, columns=['org_type'], drop_first=True)\n",
    "\n",
    "    # Convert True/False dummy variable categories to integer 0/1\n",
    "    bool_columns = df6.select_dtypes(include='bool').columns\n",
    "    df6[bool_columns] = df6[bool_columns].astype(int)\n",
    "\n",
    "    # Need to cut dataframe to earliest and latest data available for all fields. Do this by cutting rows where all values for key variables are zero\n",
    "    df6 = df6[~((df5['leaver_rate'] == 0) | (df6['joiner_rate'] == 0) | (df6['sickness_absence'] == 0))]\n",
    "\n",
    "    # transform nans into 0s\n",
    "    df6 = df6.fillna(0)\n",
    "\n",
    "    # drop duplicates\n",
    "    df6.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs = merge_ivs(annual_df1,df_r1, df_r2, df_r3,df_r4,df_r5,df_r6)\n",
    "#monthly_df_ivs = merge_ivs(monthly_df1,df_r1, df_r2, df_r3,df_r4,df_r5,df_r6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs.to_csv(f'annual_modelling_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly_df_ivs.to_csv(f'monthly_modelling_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS multiple regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the dependent variable (dv). All other fields to be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = 'leaver_rate'\n",
    "to_drop = ['month_year','org_code','staff_group',\n",
    "            'start_date','file_date','grouped_sg',                          \n",
    "           dv]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the design matrix (X) and the dependent variable (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs.reset_index(drop = True)\n",
    "X = annual_df_ivs.drop(to_drop, axis=1)\n",
    "y = annual_df_ivs[dv]\n",
    "\n",
    "y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a constant column to the design matrix\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "X.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the regression model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the regression results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the regression results summary to a DataFrame\n",
    "results_df = pd.read_html(results.summary().tables[1].as_html(), header=0, index_col=0)[0]\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "results_df.to_csv(\"annual_regression_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values(by='coef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dv = 'leaver_rate'\n",
    "# to_drop = ['month_year','org_code','staff_group','annual_sickness_absence',\n",
    "#            dv]\n",
    "\n",
    "# monthly_df_ivs.reset_index(drop = True)\n",
    "# X = monthly_df_ivs.drop(to_drop, axis=1)\n",
    "# y = monthly_df_ivs[dv]\n",
    "\n",
    "# #y = y.dropna()\n",
    "\n",
    "# y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add a constant column to the design matrix\n",
    "# X = sm.add_constant(X)\n",
    "\n",
    "# X.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the regression model\n",
    "model = sm.OLS(y, X)\n",
    "monthly_results = model.fit()\n",
    "\n",
    "# Print the regression results\n",
    "print(monthly_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the regression results summary to a DataFrame\n",
    "monthly_results_df = pd.read_html(monthly_results.summary().tables[1].as_html(), header=0, index_col=0)[0]\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "monthly_results_df.to_csv(\"monthly_regression_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fbfea1dd5a17f80dff8df3ba641602c59e31ce1a55b82aea18e6894ff3c71a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
