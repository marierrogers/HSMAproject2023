{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing for dashboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in and process dependent variable - turnover"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read turnover data from csv into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual and monthly data\n",
    "\n",
    "annual_url = '../01_data/processed_annual_turnover.csv'\n",
    "#monthly_url = '../01_data/processed_monthly_turnover.csv'\n",
    "\n",
    "annual_df = pd.read_csv(annual_url, parse_dates=['month_year'])\n",
    "annual_df = annual_df.drop(['n'],axis=1)\n",
    "#annual_df.info()\n",
    "\n",
    "#monthly_df = pd.read_csv(monthly_url, parse_dates=['month_year'])\n",
    "#monthly_df = monthly_df.drop(['n'],axis=1)\n",
    "annual_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to clean and process DV dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dv(df):\n",
    "    # drop org_type because it has historic variation which is creating duplicates when mapping later\n",
    "    df.drop('org_type', axis=1, inplace=True) \n",
    "    # add leaver and joiner rates column\n",
    "    df['leaver_rate'] = df['leave_FTE']/df['denom_FTE_average']\n",
    "    df['joiner_rate'] = df['join_FTE']/df['denom_FTE_average']\n",
    "\n",
    "    # replace inf values with nan (can happen with rate calcs)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df1 = clean_dv(annual_df)\n",
    "#monthly_df1 = clean_dv(monthly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df1 = annual_df1.sort_values(by='month_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut first year of dataframe as this only contains start denoms values\n",
    "annual_df2 = annual_df1[annual_df1['month_year'] >= (annual_df1['month_year'].min()) + pd.DateOffset(years=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data show the full time equivalent (FTE) and headcount (HC) number of leavers by organisation and staff group for the previous 12-month period from the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df2['staff_group'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load independent variable 1 - local unemployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data about local unemployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r1 = '../01_data/ONS_localunemployment_monthly.csv'\n",
    "df_r1 = pd.read_csv(url_r1, parse_dates=['Date'])\n",
    "\n",
    "#df_r1.drop(['thousands'],axis=1,inplace=True)\n",
    "df_r1.rename(columns={'%':'local_unemployment','Date':'month_year',\n",
    "                      'NHSE region name':'region_name'},inplace=True)\n",
    "df_r1 = df_r1.sort_values('month_year')\n",
    "df_r1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IV 2 - sickness absence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data about sickness absence to use as second regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r2 = '../01_data/data_wrangling/sickness_benchmarking.csv'\n",
    "df_r2 = pd.read_csv(url_r2, parse_dates=['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2['BENCHMARK_GROUP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trust_types_todrop = ['Clinical Commissioning Group','Integrated Care Board']\n",
    "df_r2 = df_r2[~df_r2['CLUSTER_GROUP'].isin(trust_types_todrop)]\n",
    "df_r2 = df_r2.drop(['BENCHMARK_GROUP','ORG_NAME',\n",
    "                    'NHSE_REGION_CODE','CLUSTER_GROUP','file_date'],axis=1)\n",
    "df_r2.rename(columns={'ORG_CODE':'org_code','DATE':'month_year',\n",
    "                      'NHSE_REGION_NAME':'region_name','STAFF_GROUP':'staff_group',\n",
    "                      'FTE_DAYS_LOST':'fte_days_lost','FTE_DAYS_AVAILABLE':'fte_days_available'},inplace=True)\n",
    "merge_cols = ['month_year', 'org_code','region_name','staff_group']\n",
    "df_r2['sickness_absence'] = df_r2['fte_days_lost']/df_r2['fte_days_available']\n",
    "df_r2 = df_r2.reset_index(drop=True)\n",
    "df_r2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 12-month rolling sickness absence column for use with annual turnover data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2['month_year'] = pd.to_datetime(df_r2['month_year'])\n",
    "\n",
    "# Sort the DataFrame by organisation, staff_group, and month\n",
    "df_r2.sort_values(by=['org_code', 'staff_group', 'month_year'], inplace=True)\n",
    "\n",
    "# Calculate the rolling sums for days lost and days available\n",
    "df_r2['rolling_days_lost'] = df_r2.groupby(['org_code', \n",
    "                        'staff_group'])['fte_days_lost'].rolling(window=12, min_periods=1).sum().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "df_r2['rolling_days_available'] = df_r2.groupby(['org_code', \n",
    "                        'staff_group'])['fte_days_available'].rolling(window=12, min_periods=1).sum().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# Calculate the rolling sickness absence rate\n",
    "df_r2['annual_sickness_absence'] = df_r2['rolling_days_lost'] / df_r2['rolling_days_available']\n",
    "\n",
    "# drop fte_days_lost fte_days_available, rolling_days_available and rolling_days_lost columns\n",
    "#df_r2.drop(columns=['fte_days_lost', 'fte_days_available', 'rolling_days_available', 'rolling_days_lost'], inplace=True)\n",
    "\n",
    "df_r2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sorted(df_r2['staff_group'].unique())\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 4 staff groups based on average absence rate\n",
    "top_staff_groups = df_r2.groupby('staff_group')['sickness_absence'].mean().nlargest(4).index\n",
    "\n",
    "# Filter the DataFrame for the top 4 staff groups\n",
    "top_groups_df = df_r2[df_r2['staff_group'].isin(top_staff_groups)]\n",
    "\n",
    "# Pivot the DataFrame to have staff groups as columns\n",
    "pivoted_df = top_groups_df.pivot_table(index='month_year', columns='staff_group', values='sickness_absence', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r2.groupby(['month_year', 'staff_group'])['sickness_absence'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting each staff group as a separate line\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for staff_group in pivoted_df.columns:\n",
    "    plt.plot(pivoted_df.index, pivoted_df[staff_group], marker='o', linestyle='-', label=staff_group)\n",
    "\n",
    "plt.title('Sickness Absence Rate Over Time for All Staff Groups')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Absence Rate (%)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IV 3 - reasons for sickness absence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data about reasons for sickness absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r3 = '../01_data/sickness_absence_reason_pivot.csv'\n",
    "df_r3 = pd.read_csv(url_r3, parse_dates=['Date'])\n",
    "#df_r3 = df_r3.drop(['FTE days lost'],axis=1)\n",
    "df_r3.rename(columns={'Date':'month_year','Staff group':'staff_group'},inplace=True)\n",
    "#df_r2 = df_r2.reset_index(drop=True)\n",
    "\n",
    "# drop least frequent reasons for absence\n",
    "df_r3 = df_r3.drop(['substance_abus','asthma',\n",
    "                    'dental','blood_disorder','endocrine',\n",
    "                    'eye','skin_disorders','nervous_system'],axis=1)\n",
    "\n",
    "# Replace NaN values with 0 \n",
    "df_r3 = df_r3.fillna(0)\n",
    "df_r3.info()\n",
    "# national level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r3.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IV 4 and 5 - staff vacancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_sg_ref = '../01_data/ref_sg_vacancy.csv'\n",
    "df_sg_ref = pd.read_csv(url_sg_ref)\n",
    "df_sg_ref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r4 = '../01_data/vacancy_ESR.csv'\n",
    "df_r4 = pd.read_csv(url_r4,parse_dates=['month_year'],dayfirst=True)\n",
    "\n",
    "df_r4 = df_r4.drop(['Published month','Published quarter','England'],axis=1)\n",
    "df_r4.rename(columns={'NWD Staff Group':'vacancy_sg','NHS England region':'region_name',\n",
    "                        'Vacancy Wte':'vacancy_FTE'},inplace=True)\n",
    "\n",
    "df_r4 = df_r4.fillna(0)\n",
    "\n",
    "# Remove code in brackets\n",
    "df_r4['region_name'] = df_r4['region_name'].str[:-6].str.rstrip()\n",
    "\n",
    "# Add staff groupings to match other datasets\n",
    "df_r4 = pd.merge(df_r4, df_sg_ref, on='vacancy_sg',how='left')\n",
    "\n",
    "df_r4 = df_r4.drop(['all'],axis=1)\n",
    "\n",
    "df_r4.info()\n",
    "\n",
    "# regional level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r5 = '../01_data/vacancy_TRAC.csv'\n",
    "df_r5 = pd.read_csv(url_r5,parse_dates=['month_year'],dayfirst=True)\n",
    "\n",
    "df_r5 = df_r5.drop(['Published month','Published quarter','England'],axis=1)\n",
    "\n",
    "df_r5.rename(columns={'NWD Staff Group':'vacancy_sg','NHS England region':'region_name',\n",
    "                        'Advertised FTE':'advertised_FTE'},inplace=True)\n",
    "\n",
    "df_r5 = df_r5.fillna(0)\n",
    "\n",
    "# Remove region code in brackets\n",
    "df_r5['region_name'] = df_r5['region_name'].str[:-6].str.rstrip()\n",
    "\n",
    "# Add staff groupings to match other datasets\n",
    "df_r5 = pd.merge(df_r5, df_sg_ref, on='vacancy_sg',how='left')\n",
    "\n",
    "df_r5 = df_r5.drop(['all'],axis=1)\n",
    "\n",
    "# regional level\n",
    "\n",
    "df_r5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IV 6 - Reasons for leaving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are only quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_r6 = '../01_data/rfl_dec22.csv'\n",
    "df_r6 = pd.read_csv(url_r6,parse_dates=['month_year'],dayfirst=True)\n",
    "\n",
    "#df_r6 = df_r6.drop(['financial_year','quarter'],axis=1)\n",
    "\n",
    "# df_r5.rename(columns={'NWD Staff Group':'staff_group','NHS England region':'region_name',\n",
    "#                         'Advertised FTE':'advertised_FTE'},inplace=True)\n",
    "\n",
    "# df_r5 = df_r5.fillna(0)\n",
    "\n",
    "# # Remove region code in brackets\n",
    "# df_r5['region_name'] = df_r5['region_name'].str[:-6].str.rstrip()\n",
    "# # regional level\n",
    "\n",
    "# shortern column names and add %_ at beginning\n",
    "df_r6.rename(columns={'Death in service':'%_death_in_service','Dismissal':'%_dismissal',\n",
    "                        'End of fixed term':'%_end_of_ft','Flexibility':'%_flexibility',\n",
    "                        'Health':'%_health','Incompatible working relationships':'%_incompat_relations',\n",
    "                        'Other':'%_other', 'Pay/Reward':'%_pay_reward', 'Pregnancy':'%_pregnancy',\n",
    "                        'Progression/CPD':'%_progression_cpd','Relocation':'%_relocation',\n",
    "                        'Retirement':'%_retirement','Unknown':'%_unknown','Work/Life Balance':'%_work_life_balance',\n",
    "                        'Workforce Transformation':'%_workforce_transform'},inplace=True)\n",
    "\n",
    "df_r6['%_other'] = pd.to_numeric(df_r6['%_other'], errors='coerce')\n",
    "df_r6['%_unknown'] = pd.to_numeric(df_r6['%_unknown'], errors='coerce')\n",
    "df_r6['%_workforce_transform'] = pd.to_numeric(df_r6['%_workforce_transform'], errors='coerce')\n",
    "\n",
    "df_r6.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge IV dfs to main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ivs(df, df_r1, df_r2, df_r3,df_r4,df_r5,df_r6):\n",
    "    # local unemployment rate\n",
    "    df1 = pd.merge(df, df_r1, on=['month_year', 'region_name'],how='left')\n",
    "    df1 = df1.sort_values('month_year')\n",
    "\n",
    "    # sickness absence\n",
    "    r2_merge_cols = ['month_year', 'org_code','region_name','staff_group']\n",
    "    df2 = pd.merge(df1, df_r2, on=r2_merge_cols,how='left')\n",
    "    #df2.drop_duplicates(subset=r2_merge_cols)\n",
    "\n",
    "    # reason for sickness absence\n",
    "    r3_merge_cols = ['month_year','staff_group']\n",
    "    df3 = pd.merge(df2, df_r3, on=r3_merge_cols,how='left')\n",
    "\n",
    "    # vacancy\n",
    "    r4_merge_cols = ['month_year','region_name','staff_group']\n",
    "\n",
    "    df4 = pd.merge(df3, df_r4, on=r4_merge_cols,how='left')\n",
    "\n",
    "    # df4.drop(columns=['vacancy_sg'], inplace=True)\n",
    "\n",
    "    # df4['vacancy_rate'] = df4['vacancy_FTE'] / df4['sip_FTE_region']\n",
    "\n",
    "    # advertised vacancy\n",
    "    df5 = pd.merge(df4, df_r5, on=r4_merge_cols,how='left')\n",
    "\n",
    "    # df5.drop(columns=['vacancy_sg'], inplace=True)\n",
    "\n",
    "    # df5['advertised_rate'] = df5['advertised_FTE'] / df5['sip_FTE_region']\n",
    "\n",
    "    # df5.drop(columns=['sip_FTE_region','advertised_FTE','vacancy_FTE'], inplace=True)\n",
    "\n",
    "    # reasons for leaving\n",
    "    df6 = pd.merge(df5, df_r6, on='month_year',how='left')\n",
    "\n",
    "    # Need to cut dataframe to earliest and latest data available for all fields. Do this by cutting rows where all values for key variables are zero\n",
    "    df6 = df6[~((df5['leaver_rate'] == 0) | (df6['joiner_rate'] == 0) | (df6['sickness_absence'] == 0))]\n",
    "    max_date = df6['month_year'].max()\n",
    "    min_date = df6['month_year'].min()\n",
    "    # Calculate the difference in months\n",
    "    delta = relativedelta(max_date, min_date)\n",
    "    # Extract the number of months\n",
    "    months_difference = delta.years * 12 + delta.months\n",
    "    print(f\"After trimming, the dataframe ranges from {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}, \"\n",
    "           f\"giving {months_difference} months of data\")\n",
    "    # transform nans into 0s\n",
    "    df6 = df6.fillna(0)\n",
    "\n",
    "    # drop duplicates\n",
    "    df6.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df6\n",
    "#    return max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs = merge_ivs(annual_df1,df_r1, df_r2, df_r3,df_r4,df_r5,df_r6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sickness_absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly_df_ivs = merge_ivs(monthly_df1,df_r1, df_r2, df_r3,df_r4,df_r5,df_r6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_df_ivs.to_csv(f'annual_dash_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly_df_ivs.to_csv(f'monthly_dash_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fbfea1dd5a17f80dff8df3ba641602c59e31ce1a55b82aea18e6894ff3c71a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
