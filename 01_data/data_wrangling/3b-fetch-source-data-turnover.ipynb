{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to scrape published data from NHSD webpages and output a collated list of CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_url = 'https://digital.nhs.uk/data-and-information/publications/statistical/nhs-workforce-statistics'\n",
    "file_source_url = 'https://digital.nhs.uk'\n",
    "\n",
    "response = requests.get(check_url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "past_links = soup.find( id=\"past-publications\").find_all(href=re.compile(\"publications/statistical/nhs-workforce-statistics/\"))\n",
    "latest_link = soup.find( id=\"latest-statistics\").find_all(href=re.compile(\"publications/statistical/nhs-workforce-statistics/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_links.append(latest_link[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_sub_url(thesoup, data_type, file_types = ['csv','xlsx'], hierachy = True):\n",
    "    # if hierachy is set to True then only return results in order of file_types\n",
    "\n",
    "    test = False\n",
    "\n",
    "    if test:\n",
    "        print(f\"data type is: {data_type}\")\n",
    "        print(file_types)\n",
    "        #print(soup)\n",
    "\n",
    "    csv_search = thesoup.find_all(href=re.compile(\"\\.csv$\"))\n",
    "    xlsx_search = thesoup.find_all(href=re.compile(\"\\.xlsx$\"))\n",
    "    zip_search = thesoup.find_all(href=re.compile(\"\\.zip$\"))\n",
    "\n",
    "    if test:\n",
    "        print(zip_search)\n",
    "        \n",
    "    file_dict = {\n",
    "        'csv': csv_search,\n",
    "        'xlsx': xlsx_search,\n",
    "        'zip' : zip_search\n",
    "    }\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for x in file_types:\n",
    "        res = [y['href'] for y in file_dict[x] if data_type in y['href'].lower()]\n",
    "        result.extend(res)    \n",
    "        if test:\n",
    "            print(res)\n",
    "        if hierachy and len(result) > 0:\n",
    "            break\n",
    "    \n",
    "\n",
    "        \n",
    "    try:\n",
    "        return(result)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_stats_urls(url_string, file_source_url, data_types, file_types = ['csv', 'xlsx']):\n",
    "    test = False\n",
    "    \n",
    "    x = url_string.split('/')\n",
    "    if test:\n",
    "        print(x[-1])\n",
    "\n",
    "    date_string = x[-1]\n",
    "    regex_date_string = [m.group() for m in re.finditer(\"((january|february|march|april|may|june|july|august|september|october|november|december)-\\d{4})\", date_string)]\n",
    "\n",
    "    if test:\n",
    "        print(regex_date_string)\n",
    "\n",
    "    result_dict = {}\n",
    "\n",
    "    try:\n",
    "        # Retrieve find date, which takes account of URLs with a date range within\n",
    "        formatted_date = datetime.strptime(regex_date_string[-1], \"%B-%Y\")\n",
    "        if test:\n",
    "            print(formatted_date)\n",
    "        full_url = file_source_url+url_string\n",
    "        if test:\n",
    "            print(full_url)\n",
    "        response = requests.get(full_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        result_list = []\n",
    "\n",
    "        for data_type in data_types:\n",
    "            if test:\n",
    "                print(f\"checking {data_type}\")\n",
    "                \n",
    "            dt = retrieve_sub_url(soup, data_type, file_types)\n",
    "            \n",
    "            if test:\n",
    "                print(dt)\n",
    "                \n",
    "            result_list.append(dt)\n",
    "            \n",
    "        dictionary = dict(zip(data_types, result_list))\n",
    "        dictionary.update({\n",
    "            'the_date': formatted_date\n",
    "        })\n",
    "\n",
    "        #print(dictionary)\n",
    "        return dictionary\n",
    "    except:\n",
    "        print('Could not format date')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_links[1:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing \n",
    "# reason_stats_urls_list = []\n",
    "# test_links_href  = \"/data-and-information/publications/statistical/nhs-sickness-absence-rates/nhs-sickness-absence-rates-february-2017\"\n",
    "\n",
    "# get_url = retrieve_stats_urls(test_links_href, file_source_url, [\"reason\", \"rate\", \"covd\"])\n",
    "# print(get_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_urls_list = []\n",
    "\n",
    "for link in past_links:\n",
    "    #print(link['href'])\n",
    "    get_urls = retrieve_stats_urls(link['href'], file_source_url, [\"turnover\"], ['zip'])\n",
    "    if get_urls is not None:\n",
    "        stats_urls_list.append(get_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_urls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_create_dir(parent_directory, directories):\n",
    "    if not os.path.exists(parent_directory):\n",
    "        os.mkdir(parent_directory)\n",
    "\n",
    "    # Create the subdirectories if they do not exist\n",
    "    for directory in directories:\n",
    "        directory_path = os.path.join(parent_directory, directory)\n",
    "        if not os.path.exists(directory_path):\n",
    "            os.mkdir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory names\n",
    "directories = ['turnover', 'zip']\n",
    "\n",
    "# Specify the parent directory (tempdir) in the working directory\n",
    "parent_directory = 'tempdir'\n",
    "\n",
    "# # Check if the parent directory exists, and if not, create it\n",
    "check_and_create_dir(parent_directory, directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stats_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip_url(url_to_file, thedate, folder, temp_dir=\"tempdir/zip\"):\n",
    "    # We need to expand and process zip files\n",
    "    http_response = urlopen(url_to_file)\n",
    "    zipfile = ZipFile(BytesIO(http_response.read()))\n",
    "    zipfile.extractall(path=temp_dir)\n",
    "\n",
    "    # Get the files from the path provided in the OP\n",
    "    files = Path(temp_dir).glob('*.csv')\n",
    "    for f in files:\n",
    "       #print(f)\n",
    "        file_name = 'tempdir'\n",
    "        additional_type = \"\"\n",
    "\n",
    "        if \"annual\" in f.as_posix().lower():\n",
    "            rate_type = \"-annual\"\n",
    "        elif \"monthly\" in f.as_posix().lower():\n",
    "            rate_type = '-monthly'\n",
    "        additional_type = rate_type\n",
    "\n",
    "        file_name = file_name + \"/\" + folder + \"/\" + thedate.strftime(\"%Y-%m-%d\") + \"-\" + folder + additional_type + \".csv\"\n",
    "\n",
    "        #print(f.as_posix())\n",
    "        #os.rename(f\"tempdir/{f.as_posix()}\", file_name)\n",
    "        shutil.move(f.as_posix(),file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url_to_file, thedate, folder):\n",
    "\n",
    "    # Set suffix for downloaded file\n",
    "    suffix = \"csv\"\n",
    "    if \"xlsx\" in url_to_file.lower():\n",
    "        suffix = \"xlsx\"\n",
    "\n",
    "    # Set filename\n",
    "    \n",
    "    file_name = \"tempdir\"\n",
    "    additional_type = \"\"\n",
    "\n",
    "    if folder == 'rate':\n",
    "        rate_type = \"-monthly\"\n",
    "        if \"annual\" in url_to_file.lower():\n",
    "            rate_type = \"-annual\"\n",
    "        elif \"quarterly\" in url_to_file.lower():\n",
    "            rate_type = '-quarterly'\n",
    "        additional_type = rate_type\n",
    "    elif folder == \"reason\" and \"mds\" in url_to_file.lower():\n",
    "        additional_type = '-mds'\n",
    "\n",
    "    file_name = file_name + \"/\" + folder + \"/\" + thedate.strftime(\"%Y-%m-%d\") + \"-\" + folder + additional_type + \".\" + suffix\n",
    "\n",
    "    response = requests.get(url_to_file)\n",
    "\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_urls_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in stats_urls_list:\n",
    "    #print(link['href'])\n",
    "    # NOTE: Absence by reason not available before April 2019\n",
    "    if len(m['turnover']) > 0:\n",
    "        for i in m['turnover']:\n",
    "            download_zip_url(i, m['the_date'], \"turnover\")\n",
    "\n",
    "# About a minute to download all files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
