{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sickness absence scraper from NHSD published stats\n",
    "#### Load in data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Script to scrape published data from NHSD webpages and output a compiled and procesed CSV\n",
    "### of FTE days available and FTE days lost by staff group and organisation\n",
    "### Takes up to 10 minutes to run\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Allows unverified SSLs\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# The monthly publication of sickness absences sometimes vary in format (sometimes monthly \"provisional\", sometimes just monthly, sometimes quarterly)\n",
    "# These three base URLs pick up on all variations. The code iterates through all possible URL formats for each month (and quarter). Where an invalid\n",
    "# URL is created, the code will output \"Failed to access [URL]\", where it is valid it will output \"Downloaded and parsed: [URL]\". All data about \n",
    "# failed/successful access is outputted in a seperate CSV.\n",
    "\n",
    "base_urls = [\n",
    "    \"https://digital.nhs.uk/data-and-information/publications/statistical/nhs-sickness-absence-rates/{month}-{year}-provisional-statistics\",\n",
    "    \"https://digital.nhs.uk/data-and-information/publications/statistical/nhs-sickness-absence-rates/{month}-{year}\",\n",
    "    \"https://digital.nhs.uk/data-and-information/publications/statistical/nhs-sickness-absence-rates/{month1}-{year1}-to-{month2}-{year2}-provisional-statistics\"\n",
    "]\n",
    "\n",
    "accessed_data = []\n",
    "dfs = []\n",
    "\n",
    "## Function to get quarterly month values\n",
    "\n",
    "def get_month_range_quarterly(month):\n",
    "    quarters = {\n",
    "        \"january\": (\"january\", \"march\"),\n",
    "        \"april\": (\"april\", \"june\"),\n",
    "        \"july\": (\"july\", \"september\"),\n",
    "        \"november\": (\"november\", \"december\")\n",
    "    }\n",
    "    return quarters.get(month, (None, None))\n",
    "\n",
    "## Iterates over years and months - specify years in range (remember need to +1 to upper range)\n",
    "\n",
    "for year in range(2018, 2024):\n",
    "    for month in range(1, 13):\n",
    "        month_name = datetime(year, month, 1).strftime('%B').lower()\n",
    "\n",
    "        for base_url in base_urls:\n",
    "            ## Gets correct year for quarter\n",
    "            if \"{month1}-{year1}-to-{month2}-{year2}\" in base_url:\n",
    "                start_month, end_month = get_month_range_quarterly(month_name)\n",
    "                if not start_month:\n",
    "                    continue\n",
    "                month1 = start_month\n",
    "                month2 = end_month\n",
    "                year1 = year\n",
    "                year2 = year\n",
    "                if end_month == \"december\":\n",
    "                    year2 += 1\n",
    "                url = base_url.format(month1=month1, year1=year1, month2=month2, year2=year2)\n",
    "            else:\n",
    "                url = base_url.format(month=month_name, year=year)\n",
    "\n",
    "            # Download the monthly webpage\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content of the monthly webpage\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "                # Find all the CSV links on the monthly webpage\n",
    "                csv_links = soup.select('a[href$=\".csv\"]')\n",
    "\n",
    "                # Append the accessed webpage to the accessed_data list\n",
    "                accessed_data.append({\"URL\": url, \"Status\": \"Accessed\", \"CSV Count\": len(csv_links)})\n",
    "\n",
    "                # Iterate over the CSV links and download the files\n",
    "                for link in csv_links:\n",
    "                    csv_url = link[\"href\"]\n",
    "                    # Download the CSV file\n",
    "                    response_csv = requests.get(csv_url)\n",
    "                    if response_csv.status_code == 200:\n",
    "                        # Read the CSV data into a DataFrame\n",
    "                        df = pd.read_csv(csv_url)\n",
    "                        # Add a new column with the downloaded URL\n",
    "                        df[\"Downloaded From\"] = csv_url\n",
    "                        # Append the downloaded data to the dfs list\n",
    "                        dfs.append(df)\n",
    "                        # Append the downloaded CSV URL to the downloaded_data list\n",
    "                        accessed_data.append({\"URL\": csv_url, \"Status\": \"Downloaded\"})\n",
    "                        print(f\"Downloaded and parsed: {csv_url}\")\n",
    "                    else:\n",
    "                        # Append the failed CSV URL to the downloaded_data list\n",
    "                        accessed_data.append({\"URL\": csv_url, \"Status\": \"Failed\"})\n",
    "                        print(f\"Failed to download CSV from {csv_url}\")\n",
    "                break  # Exit the loop if CSV files were found and downloaded\n",
    "            else:\n",
    "                # Append the failed webpage to the accessed_data list\n",
    "                accessed_data.append({\"URL\": url, \"Status\": \"Failed\", \"CSV Count\": 0})\n",
    "                print(f\"Failed to access webpage: {url}\")\n",
    "\n",
    "accessed_df = pd.DataFrame(accessed_data)\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Optional: Save the accessed_df to separate CSV file\n",
    "accessed_df.to_csv(\"tempdir/accessed_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and save sickness absence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the 'Downloaded From' for cells containing \"benchmarking\" because that picks up on\n",
    "# sickness absence benchmarking data which is in the format we want. \n",
    "absence_df = combined_df[combined_df['Downloaded From'].str.contains('benchmarking')].dropna(axis = 1, \n",
    "                                                                                             how = 'all').dropna(axis = 0, \n",
    "                                                                                             how = 'all').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge 'Month' and 'DATE' columns into a single column 'Date'\n",
    "absence_df['Date'] = absence_df['Month'].combine_first(absence_df['DATE'])\n",
    "\n",
    "# Drop old dates columns\n",
    "absence_df = absence_df.drop(['Month','DATE'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Month' column to datetime format\n",
    "absence_df['Date'] = pd.to_datetime(absence_df['Date'], errors='coerce')\n",
    "\n",
    "# Convert Month column dates to the format 'YYYY-MM-DD'\n",
    "absence_df['Date'] = absence_df['Date'].dt.to_period('M').dt.to_timestamp()\n",
    "sorted(absence_df['Date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing for all columns containing same data category but different names\n",
    "columns_to_merge = {\n",
    "    'NHSE region code': 'NHSE_REGION_CODE',\n",
    "    'NHSE region name': 'NHSE_REGION_NAME',\n",
    "    'Org code': 'ORG_CODE',\n",
    "    'Org name': 'ORG_NAME',\n",
    "    'FTE days lost': 'FTE_DAYS_LOST',\n",
    "    'FTE days available': 'FTE_DAYS_AVAILABLE',\n",
    "    'Sickness absence rate (%)': 'SICKNESS_ABSENCE_RATE_PERCENT',\n",
    "    'Staff group': 'STAFF_GROUP',\n",
    "    'Cluster group': 'CLUSTER_GROUP',\n",
    "    'Benchmark group': 'BENCHMARK_GROUP',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the columns to merge\n",
    "for column, matching_column in columns_to_merge.items():\n",
    "    # Check if both columns exist in the dataframe\n",
    "    if column in absence_df.columns and matching_column in absence_df.columns:\n",
    "        # Merge the columns by filling the missing values\n",
    "        absence_df[column] = absence_df[column].fillna(absence_df[matching_column])\n",
    "\n",
    "# Drop the matching columns so only the merged column remains\n",
    "absence_df = absence_df.drop(columns_to_merge.values(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up and simplify data frame\n",
    "to_drop = ['Downloaded From','Tm End Date','ICS_CODE','ICS_NAME','HEE region code',\n",
    "         'HEE region name','Sickness absence rate (%)']\n",
    "replace_dict_region = {'South East of England':'South East',\n",
    "                'South West of England':'South West'}\n",
    "replace_dict_staff = {'All staff':'All staff groups',\n",
    "                'HCHS Doctors':'HCHS doctors (exc. junior Drs)',\n",
    "                'HCHS doctors':'HCHS doctors (exc. junior Drs)'}\n",
    "df = absence_df.sort_values('Date')\n",
    "df['NHSE region name'] = df['NHSE region name'].replace(replace_dict_region)\n",
    "df['Staff group'] = df['Staff group'].replace(replace_dict_staff)\n",
    "df.drop(to_drop,axis=1, inplace=True)\n",
    "order = ['Date','Org code','Org name','NHSE region code','NHSE region name','Cluster group','Benchmark group',\n",
    "         'Staff group','FTE days lost','FTE days available']\n",
    "df = df.drop_duplicates()\n",
    "df = df[order].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where 'fte days available' is nan as assume no data available\n",
    "df = df.dropna(subset=['FTE days available'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for June 2022 is missing. To fill the missing values I will duplicate May 2022's valid data for June 2022 (2022-06-01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the data corresponding to 2022-05-01\n",
    "may_data = df[df['Date'] == pd.to_datetime('2022-05-01')]\n",
    "\n",
    "# Create a copy of the data with the date changed to 2022-06-01\n",
    "june_data = may_data.copy()\n",
    "june_data['Date'] = pd.to_datetime('2022-06-01')\n",
    "\n",
    "# Append the copied data to the original DataFrame\n",
    "df2 = pd.concat([df, june_data], ignore_index=True)\n",
    "\n",
    "df2_check = df2[df2['Date'] == pd.to_datetime('2022-06-01')]\n",
    "df2_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('../sickness_absence.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and save sickness absence *reason* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the 'Downloaded From' for cells containing \"REASON\" because that picks up on sickness absence reason data. Drop other columns that contain all NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df = combined_df[combined_df['Downloaded From'].str.contains('REASON')].dropna(axis = 1, \n",
    "                                                                                      how = 'all').dropna(axis = 0, \n",
    "                                                                                      how = 'all').reset_index(drop=True)\n",
    "\n",
    "reason_df = reason_df.drop(['Downloaded From'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df['Month'] = pd.to_datetime(reason_df['Month'])\n",
    "sorted(reason_df['Month'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach to see if we can get later data - where REASON column is populated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df2 = combined_df[combined_df['REASON'].notna()].dropna(axis = 1, how = 'all').dropna(axis = 0, \n",
    "                                                                                      how = 'all').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df2 = reason_df2.drop(['Downloaded From'], axis=1)\n",
    "reason_df2['DATE'] = pd.to_datetime(reason_df2['DATE'])\n",
    "sorted(reason_df2['DATE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this is where the rest of the data are so we need to stitch together these dfs. There is someoverlap so I will cut 2022-04 and 2022-05 from the first df. Then process them both in that same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df = reason_df[reason_df['Month'] < '2022-04-30']\n",
    "#sorted(reason_df['Month'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Month column dates to the format 'YYYY-MM-DD'\n",
    "reason_df['Month'] = reason_df['Month'].dt.to_period('M').dt.to_timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DATE column dates to the format 'YYYY-MM-DD'\n",
    "reason_df2['DATE'] = reason_df2['DATE'].dt.to_period('M').dt.to_timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df.rename(columns={'Month': 'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df2.rename(columns={'DATE': 'Date','FTE_DAYS_LOST':'FTE days lost',\n",
    "                           'STAFF_GROUP':'Staff group','REASON':'Reason'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df['Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the 'Type' column using pivot_table\n",
    "p_reason_df = pd.pivot_table(reason_df, index=['Date','Reason','Staff group'], columns=['Type'], values='FTE days', aggfunc='sum')\n",
    "\n",
    "# Reset the index\n",
    "p_reason_df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FTE days available is only recorded for all reasons (not broken down by sickness absence reason), so we can drop that column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_reason_df = p_reason_df.drop(['FTE days available'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df2 = reason_df2.drop(['FTE_DAYS_AVAILABLE'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_reason_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reason_df = pd.concat([p_reason_df, reason_df2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_reason_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import sickness absence reference table to decode reason information into a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'REF_SICK_ABSENCE_REASONS.csv'\n",
    "df_ref = pd.read_csv(url)\n",
    "df_ref.rename(columns={'Sick_Lv1_Reason':'Reason','Sick_Lv1_Description':'Description'},inplace=True) \n",
    "df_ref.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add description information to main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason = pd.merge(cat_reason_df, df_ref[['Reason','reason_short']], on='Reason',how='left')\n",
    "df_reason.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where 'Description' is nan as this equates to no reason breakdown\n",
    "df_reason = df_reason.dropna(subset=['reason_short'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop Reason column as information now in reason_short\n",
    "df_reason = df_reason.drop(['Reason'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-duplicate - now lots of duplicate rows\n",
    "df_reason = df_reason.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more staff groups in this data than in the independent variable data - need to compile HCHS doctors and then drop ones that don't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_reason['Staff group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_grades = ['Specialty Doctor',\n",
    " 'Specialty Registrar',\n",
    " 'Staff Grade',\n",
    " 'Professionally qualified clinical staff',\n",
    " 'Other and Local HCHS Doctor Grades',\n",
    " 'Hospital Practitioner / Clinical Assistant',\n",
    " 'HCHS doctors',\n",
    " 'Consultant',\n",
    " 'Core Training',\n",
    " 'Foundation Doctor Year 1',\n",
    " 'Foundation Doctor Year 2',\n",
    " 'Associate Specialist']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason['Staff group'] = df_reason['Staff group'].replace('HCHS doctors','HCHS doctors (exc. junior Drs)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_in_df_reason = df_reason['Staff group'].unique()\n",
    "sg_in_df_absence = df['Staff group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason = df_reason[df_reason['Staff group'].isin(sg_in_df_absence)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate proportion of sickness absence due each reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total days lost by staff group, reason and 'date'\n",
    "df_reason['total_days_lost'] = df_reason.groupby(['Staff group', 'Date'])['FTE days lost'].transform('sum')\n",
    "\n",
    "# Calculate the percentage of days lost for each 'staff_group', 'reason', and 'date'\n",
    "df_reason['percentage_days_lost'] = (df_reason['FTE days lost'] / df_reason['total_days_lost']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a new dataframe where the proportion of sickness absence days due to each reason is a separate column, format suitable for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_as_cols = df_reason.pivot_table(index=['Date','Staff group'], \n",
    "                                          columns='reason_short', values='percentage_days_lost', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_as_cols.reset_index(inplace=True)\n",
    "df_reason_as_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "June 2022 data is missing, as in sickness absence data. I will duplicate May 2022 data for June 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the data corresponding to 2022-05-01\n",
    "may_data = df_reason_as_cols[df_reason_as_cols['Date'] == pd.to_datetime('2022-05-01')]\n",
    "\n",
    "# Create a copy of the data with the date changed to 2022-06-01\n",
    "june_data = may_data.copy()\n",
    "june_data['Date'] = pd.to_datetime('2022-06-01')\n",
    "\n",
    "# Append the copied data to the original DataFrame\n",
    "df_reason_as_cols2 = pd.concat([df_reason_as_cols, june_data], ignore_index=True)\n",
    "\n",
    "check = df_reason_as_cols2[df_reason_as_cols2['Date'] == pd.to_datetime('2022-06-01')]\n",
    "check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_as_cols2.to_csv('../sickness_absence_reason_pivot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_reason['Staff group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staff_group = 'All staff groups'\n",
    "df_reason_sg = df_reason.loc[df_reason['Staff group'] == staff_group]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason_sg = df_reason_sg.sort_values(by='percentage_days_lost', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a grouped bar plot using seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.barplot(x='Staff group', y='percentage_days_lost', hue='reason_short', data=df_reason_sg)\n",
    "plt.title('Percentage of Days Lost by Reason and Staff Group')\n",
    "plt.xlabel('Staff Group')\n",
    "plt.ylabel('Percentage of Days Lost')\n",
    "plt.legend(title='Reason')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reason['Description']\n",
    "# Group the data by 'reason' and calculate the total days lost for each reason\n",
    "#reasons_totals = df_reason.groupby('Description')['FTE days lost'].sum().reset_index()\n",
    "\n",
    "# Sort the DataFrame by 'days_lost' in descending order\n",
    "#reasons_totals = reasons_totals.sort_values(by='FTE days lost', ascending=False)\n",
    "\n",
    "#reasons_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reason.to_csv('../sickness_absence_reason.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and save sickness absence due to COVID data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the 'Downloaded From' for cells containing \"COVID-19\" because that picks up on sickness absence reason data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_df = combined_df[combined_df['Downloaded From'].str.contains('COVID-19')].dropna(axis = 1,\n",
    "                                                                                          how = 'all').dropna(axis = 0,\n",
    "                                                                                          how = 'all').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_df = covid19_df.drop(['Downloaded From'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_df = covid19_df.dropna(subset=['FTE_DAYS_AVAILABLE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_df['DATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_df.to_csv('../covid-19_sickness_absence.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('objs.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "#     pickle.dump([absence_df, combined_df, covid19_df, df, df_reason, df_ref,p_reason_df,\n",
    "#                  reason_df], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('objs.pkl') as f:  # Python 3: open(..., 'rb')\n",
    "#    absence_df, combined_df, covid19_df, df, df_reason, df_ref,p_reason_df,reason_df = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
